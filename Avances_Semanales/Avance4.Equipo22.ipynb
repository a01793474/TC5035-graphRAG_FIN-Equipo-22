{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fd823d-c64e-4a2d-a01f-67e52191045e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings, AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from neo4j.exceptions import ClientError\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "from time import sleep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc09045-f9dc-4d05-a963-71d49ec5fd9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(r\"C:\\Users\\lriospie\\OneDrive - azureford\\Documents\\Maestria\\Integrador\\Neo4j_RAG\\.env\",verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a98868-d5ea-45e7-b66c-7f6e91ae0f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# client = AzureOpenAI(\n",
    "#     api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"), \n",
    "#     api_version=\"2023-05-15\",\n",
    "#     azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "# )\n",
    "\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY=os.getenv(\"AZURE_OPENAI_API_KEY\"), \n",
    "\n",
    "#deployment_name='text-embedding-3' #This will correspond to the custom name you chose for your deployment when you deployed a model.\n",
    "\n",
    "# Embeddings & LLM models\n",
    "embedding_dimension = 1536\n",
    "embeddings = AzureOpenAIEmbeddings(azure_deployment=\"text-embedding-3\",api_version=\"2024-02-01\",dimensions=embedding_dimension)\n",
    "\n",
    "llm = AzureChatOpenAI(azure_deployment='chat_gtp_35',api_version=\"2023-05-15\", temperature=0)\n",
    "\n",
    "# Get Neo4j credentials from environment variables\n",
    "NEO4J_URI=os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USERNAME=os.getenv(\"NEO4J_USER\")\n",
    "NEO4J_PASSWORD=os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "graph = Neo4jGraph()\n",
    "sleep(5)\n",
    "graph.query(\"MATCH (n) DETACH DELETE n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e9be0-c94a-4c3d-bb0b-2e1ad2f53877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the text file\n",
    "txt_path =\"C:/Users/lriospie/OneDrive - azureford/Documents/Maestria/Integrador/Neo4j_RAG/Entrega Modelo Final/sec_10K_data/10_K_MSFT_0000950170-23-035122.txt\"\n",
    "#loader = TextLoader(str(txt_path))\n",
    "loader = TextLoader(str(txt_path), encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "# Ingest Parent-Child node pairs\n",
    "parent_splitter = TokenTextSplitter(chunk_size=512*3, chunk_overlap=24)\n",
    "child_splitter = TokenTextSplitter(chunk_size=100*3, chunk_overlap=24)\n",
    "parent_documents = parent_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b504f-3a09-4e78-b89a-98c3bbd732a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph.query(\"MATCH (n) DETACH DELETE n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a5e34-3430-43b5-9eb3-6453ae4d7709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, parent in enumerate(parent_documents):\n",
    "    child_documents = child_splitter.split_documents([parent])\n",
    "    params = {\n",
    "        \"parent_text\": parent.page_content,\n",
    "        \"parent_id\": i,\n",
    "        \"parent_embedding\": embeddings.embed_query(parent.page_content),\n",
    "        \"children\": [\n",
    "            {\n",
    "                \"text\": c.page_content,\n",
    "                \"id\": f\"{i}-{ic}\",\n",
    "                \"embedding\": embeddings.embed_query(c.page_content),\n",
    "            }\n",
    "            for ic, c in enumerate(child_documents)\n",
    "        ],\n",
    "    }\n",
    "    # Ingest data\n",
    "    sleep(5)\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "    MERGE (p:Parent {id: $parent_id})\n",
    "    SET p.text = $parent_text\n",
    "    WITH p\n",
    "    CALL db.create.setVectorProperty(p, 'embedding', $parent_embedding)\n",
    "    YIELD node\n",
    "    WITH p \n",
    "    UNWIND $children AS child\n",
    "    MERGE (c:Child {id: child.id})\n",
    "    SET c.text = child.text\n",
    "    MERGE (c)<-[:HAS_CHILD]-(p)\n",
    "    WITH c, child\n",
    "    CALL db.create.setVectorProperty(c, 'embedding', child.embedding)\n",
    "    YIELD node\n",
    "    RETURN count(*)\n",
    "    \"\"\",\n",
    "        params,\n",
    "    )\n",
    "    # Create vector index for child\n",
    "    try:\n",
    "        graph.query(\n",
    "            \"CALL db.index.vector.createNodeIndex('parent_document', \"\n",
    "            \"'Child', 'embedding', $dimension, 'cosine')\",\n",
    "            {\"dimension\": embedding_dimension},\n",
    "        )\n",
    "    except ClientError:  # already exists\n",
    "        pass\n",
    "    # Create vector index for parents\n",
    "    try:\n",
    "        graph.refresh_schema()\n",
    "        graph.query(\n",
    "            \"CALL db.index.vector.createNodeIndex('typical_rag', \"\n",
    "            \"'Parent', 'embedding', $dimension, 'cosine')\",\n",
    "            {\"dimension\": embedding_dimension},\n",
    "        )\n",
    "    except ClientError:  # already exists\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba4cb8a-ff21-4b71-aa87-c7bf5bc40cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ingest hypothethical questions\n",
    "class Questions(BaseModel):\n",
    "    \"\"\"Generating hypothetical questions about text.\"\"\"\n",
    "\n",
    "    questions: List[str] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Generated hypothetical questions based on \" \"the information from the text\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "questions_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You are generating hypothetical questions based on the information \"\n",
    "                \"found in the text and must reply in JSON format. Make sure to provide full context in the generated \"\n",
    "                \"questions.\"\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"Use the given format to generate hypothetical questions from the\"\n",
    "                \"following input: {input}\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_chain = questions_prompt | llm.with_structured_output(Questions.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6f3ca-736e-4099-8d73-58f27ca97a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_questions_data(questions_data):\n",
    "    if 'questions' not in questions_data:\n",
    "        return []\n",
    "\n",
    "    cleaned_questions = []\n",
    "    for entry in questions_data['questions']:\n",
    "        if isinstance(entry, dict) and 'question' in entry:\n",
    "            cleaned_questions.append(entry)\n",
    "        elif isinstance(entry, str):\n",
    "            cleaned_questions.append({'question': entry})\n",
    "        else:\n",
    "            # Handle cases where the entry is not a string or doesn't contain the 'question' key\n",
    "            print(f\"Invalid entry found and skipped: {entry}\")\n",
    "    # Convert the cleaned_questions list into a pandas DataFrame\n",
    "    df = pd.DataFrame(cleaned_questions)\n",
    "\n",
    "    # Generate a unique file ID using the current timestamp\n",
    "    import datetime\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    file_id = f\"questions_{timestamp}\"\n",
    "\n",
    "    # Save the DataFrame into a CSV file with the unique file ID\n",
    "    df.to_csv(f\"{file_id}.csv\", index=False)\n",
    "\n",
    "    return cleaned_questions\n",
    "\n",
    "for i, parent in enumerate(parent_documents):\n",
    "    questions_data = question_chain.invoke(parent.page_content)\n",
    "    cleaned_questions_list = clean_questions_data(questions_data)\n",
    "    print(cleaned_questions_list)\n",
    "    if 'questions' in questions_data:\n",
    "        questions_list = questions_data['questions']  # Access the 'questions' list\n",
    "        params = {\n",
    "            \"parent_id\": i,\n",
    "            \"questions\": []\n",
    "        }\n",
    "        \n",
    "        for iq, q in enumerate(cleaned_questions_list):  # Use the cleaned questions list\n",
    "            if isinstance(q, dict) and 'question' in q:\n",
    "                try:\n",
    "                    question_text = q['question']\n",
    "                    embedding = embeddings.embed_query(question_text)\n",
    "                    params[\"questions\"].append({\"text\": question_text, \"id\": f\"{i}-{iq}\", \"embedding\": embedding})\n",
    "                except Exception as e:\n",
    "                    print(f\"Error embedding question {q}: {e}\")\n",
    "            else:\n",
    "                print(f\"Skipping invalid question entry: {q}\")\n",
    "        \n",
    "        sleep(5)\n",
    "        graph.query(\n",
    "            \"\"\"\n",
    "        MERGE (p:Parent {id: $parent_id})\n",
    "        WITH p\n",
    "        UNWIND $questions AS question\n",
    "        CREATE (q:Question {id: question.id})\n",
    "        SET q.text = question.text\n",
    "        MERGE (q)<-[:HAS_QUESTION]-(p)\n",
    "        WITH q, question\n",
    "        CALL db.create.setVectorProperty(q, 'embedding', question.embedding)\n",
    "        YIELD node\n",
    "        RETURN count(*)\n",
    "        \"\"\",\n",
    "            params,\n",
    "        )\n",
    "        # Create vector index\n",
    "        try:\n",
    "            graph.refresh_schema()\n",
    "            graph.query(\n",
    "                \"CALL db.index.vector.createNodeIndex('hypothetical_questions', \"\n",
    "                \"'Question', 'embedding', $dimension, 'cosine')\",\n",
    "                {\"dimension\": embedding_dimension},\n",
    "            )\n",
    "        except ClientError:  # already exists\n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d1a9d-e1a0-4594-9383-59a5620defbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest summaries\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You are generating concise and accurate summaries based on the \"\n",
    "                \"information found in the text. Do not miss in the summaries any information\" \n",
    "                \"related with numbers such as dates, costs, rates, porcentages or any sort of numerical values \"\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\"Generate a summary of the following input: {question}\\n\" \"Summary:\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "summary_chain = summary_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a4613-b6c2-454d-9851-4bf350c54303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, parent in enumerate(parent_documents):\n",
    "    summary = summary_chain.invoke({\"question\": parent.page_content}).content\n",
    "    params = {\n",
    "        \"parent_id\": i,\n",
    "        \"summary\": summary,\n",
    "        \"embedding\": embeddings.embed_query(summary),\n",
    "    }\n",
    "    \n",
    "    sleep(5)\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "    MERGE (p:Parent {id: $parent_id})\n",
    "    MERGE (p)-[:HAS_SUMMARY]->(s:Summary)\n",
    "    SET s.text = $summary\n",
    "    WITH s\n",
    "    CALL db.create.setVectorProperty(s, 'embedding', $embedding)\n",
    "    YIELD node\n",
    "    RETURN count(*)\n",
    "    \"\"\",\n",
    "        params,\n",
    "    )\n",
    "    # Create vector index\n",
    "    try:\n",
    "        graph.query(\n",
    "            \"CALL db.index.vector.createNodeIndex('summary', \"\n",
    "            \"'Summary', 'embedding', $dimension, 'cosine')\",\n",
    "            {\"dimension\": embedding_dimension},\n",
    "        )\n",
    "    except ClientError:  # already exists\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42ed9b-b438-4478-b3f3-f091f9525eb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "\n",
    "\n",
    "# Typical RAG retriever\n",
    "\n",
    "typical_rag = Neo4jVector.from_existing_index(\n",
    "    embeddings, index_name=\"typical_rag\"\n",
    ")\n",
    "\n",
    "# Parent retriever\n",
    "\n",
    "parent_query = \"\"\"\n",
    "MATCH (node)<-[:HAS_CHILD]-(parent)\n",
    "WITH parent, max(score) AS score // deduplicate parents\n",
    "RETURN parent.text AS text, score, {} AS metadata LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "parent_vectorstore = Neo4jVector.from_existing_index(\n",
    "    embeddings,\n",
    "    index_name=\"parent_document\",\n",
    "    retrieval_query=parent_query,\n",
    ")\n",
    "\n",
    "# Hypothetic questions retriever\n",
    "\n",
    "hypothetic_question_query = \"\"\"\n",
    "MATCH (node)<-[:HAS_QUESTION]-(parent)\n",
    "WITH parent, max(score) AS score // deduplicate parents\n",
    "RETURN parent.text AS text, score, {} AS metadata\n",
    "\"\"\"\n",
    "\n",
    "hypothetic_question_vectorstore = Neo4jVector.from_existing_index(\n",
    "    embeddings,\n",
    "    index_name=\"hypothetical_questions\",\n",
    "    retrieval_query=hypothetic_question_query,\n",
    ")\n",
    "# Summary retriever\n",
    "\n",
    "summary_query = \"\"\"\n",
    "MATCH (node)<-[:HAS_SUMMARY]-(parent)\n",
    "WITH parent, max(score) AS score // deduplicate parents\n",
    "RETURN parent.text AS text, score, {} AS metadata\n",
    "\"\"\"\n",
    "\n",
    "summary_vectorstore = Neo4jVector.from_existing_index(\n",
    "    embeddings,\n",
    "    index_name=\"summary\",\n",
    "    retrieval_query=summary_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5bdd3-be56-4927-a075-4a3d62a5423e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = hypothetic_question_vectorstore.similarity_search(\n",
    "    \"What is The Department of Justice recommendation on marijuana\"\n",
    ")\n",
    "print(response[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319aa15c-64ca-492a-81e1-6697a2386d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "graph.refresh_schema()\n",
    "\n",
    "vector_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=typical_rag.as_retriever()\n",
    ")\n",
    "\n",
    "vector_qa.invoke(\n",
    "     \"Which other drugs appear along with marijuana in the Schedule I\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9fe107-6bee-47d2-ac5d-876f767ba4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.refresh_schema()\n",
    "\n",
    "vector_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=typical_rag.as_retriever()\n",
    ")\n",
    "\n",
    "vector_qa.invoke(\n",
    "    #\"What Wexler previously told to USA TODAY\"\n",
    "    \"What experts  exposed to USA TODAY on marijuana\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d3395-99ee-4755-8d81-e91cb7df310b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "# graph = Neo4jGraph()\n",
    "graph.refresh_schema()\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm = AzureChatOpenAI(azure_deployment='Chat_gpt_4',api_version=\"2023-05-15\", temperature=0),\n",
    "    qa_llm = AzureChatOpenAI(azure_deployment='chat_gtp_35',api_version=\"2023-05-15\", temperature=0), graph=graph, verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26494579-3163-4572-a250-9be0807ae114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph.refresh_schema()\n",
    "cypher_chain.invoke(\n",
    "     \"How many times Schedule III is mentioned within the text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce02c064-5d6e-45d3-954d-8a05a5ff91e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "graph = Neo4jGraph()\n",
    "graph.refresh_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29d7504-4edd-4ec9-b788-b685d1cb652d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Tasks\",\n",
    "        func=vector_qa.run,\n",
    "        description=\"\"\"Useful to reply to most of the queries as long as any sort of aggregation is not requested.\n",
    "        Use full question as input.\n",
    "        \"\"\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Graph\",\n",
    "        func=cypher_chain.run,\n",
    "        description=\"\"\"Usfeful when any sort of aggregation is requested in the query.\n",
    "        Use full question as input.\n",
    "        \"\"\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "mrkl = initialize_agent(\n",
    "    tools, \n",
    "    AzureChatOpenAI(azure_deployment='Chat_gpt_4',api_version=\"2023-05-15\", temperature=0),\n",
    "    agent=AgentType.OPENAI_FUNCTIONS, verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a046490e-0d7e-4341-be28-cd47524a5674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = mrkl.invoke(\"How many times Schedule III is mentioned within the text\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaafff0-b7c9-44c5-87fb-382fcb03bc37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = mrkl.invoke(\"Who might be affected with IQ loss due to marijuana consumption\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
