{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM RAG GENERATION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab\n",
    "#!pip install huggingface_hub\n",
    "#!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import os\n",
    "import torch\n",
    "from timeit import default_timer as timer\n",
    "import gc\n",
    "from string import Template\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "# For Colab\n",
    "#from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab\n",
    "# hugging face auth\n",
    "# notebook_login() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case cuda ram memory overflow\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\"\n",
    "def reset_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    #print(torch.cuda.memory_summary(device=None, abbreviated=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e43a764e94d490185e39310ce9aa216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, shiver me timbers! Me name be Captain Chat, the scurviest pirate chatbot to ever sail the Seven Seas! Me be here to swab the decks with me trusty keyboard, answerin' yer questions and tellin' tales of me adventures on the high seas. So hoist the colors, me hearty, and let's set sail fer a swashbucklin' good time!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32 #torch.bfloat16\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the output in the required format:\n",
      "\n",
      "{\n",
      "    \"entities\": [\n",
      "        {\"label\": \"Person\", \"id\": \"sarahjohnson\", \"name\": \"Sarah Johnson\"},\n",
      "        {\"label\": \"Person\", \"id\": \"davidpatel\", \"name\": \"David Patel\"},\n",
      "        {\"label\": \"Person\", \"id\": \"amandarodriguez\", \"name\": \"Amanda Rodriguez\"},\n",
      "        {\"label\": \"Person\", \"id\": \"jasonmitchell\", \"name\": \"Jason Mitchell\"},\n",
      "        {\"label\": \"Person\", \"id\": \"emilyturner\", \"name\": \"Emily Turner\"},\n",
      "        {\"label\": \"Person\", \"id\": \"michaelclark\", \"name\": \"Michael Clark\"},\n",
      "        {\"label\": \"Person\", \"id\": \"jessicawhite\", \"name\": \"Jessica White\"},\n",
      "        {\"label\": \"Person\", \"id\": \"danielbrown\", \"name\": \"Daniel Brown\"},\n",
      "        {\"label\": \"Person\", \"id\": \"oliviamartinez\", \"name\": \"Olivia Martinez\"},\n",
      "        {\"label\": \"Person\", \"id\": \"williamlee\", \"name\": \"William Lee\"},\n",
      "        {\"label\": \"\n",
      "Pipeline completed in 3261.1733299000007 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful IT-project and account management expert who extracts information from documents.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nFrom the list of people below, extract the following Entities & relationships described in the mentioned format \\n0. ALWAYS FINISH THE OUTPUT. Never send partial responses\\n1. First, look for these Entity types in the text and generate as comma-separated format similar to entity type.\\n   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. Do not create new entity types that aren\\'t mentioned below. You will have to generate as many entities as needed as per the types below:\\n    Entity Types:\\n    label:\\'Project\\',id:string,name:string;summary:string //Project mentioned in the profile; `id` property is the full lowercase name of the project, with no capital letters, special characters, spaces or hyphens.\\n    label:\\'Technology\\',id:string,name:string //Technology Entity, as listed in the \"skills\"-section of every person; `id` property is the name of the technology, in camel-case.\\n    \\n3. Next generate each relationships as triples of head, relationship and tail. To refer the head and tail entity, use their respective `id` property. Relationship property should be mentioned within brackets as comma-separated. They should follow these relationship types below. You will have to generate as many relationships as needed as defined below:\\n    Relationship types:\\n    person|HAS_SKILLS|technology \\n    project|HAS_PEOPLE|person\\n\\n\\nThe output should look like :\\n{\\n    \"entities\": [{\"label\":\"Person\",\"id\":string,\"name\":string}],\\n    \"relationships\": [\"projectid|HAS_PEOPLE|personid\"]\\n}\\n\\nCase Sheet:\\nFull Name: Sarah Johnson\\nSkills: Machine Learning, Data Analytics, Azure, Python\\nProjects: BetaHealth Secure Healthcare Data Analytics Platform on Azure\\n\\nFull Name: David Patel\\nSkills: AWS, Cloud Computing, DevOps, Data Warehousing\\nProjects: \\n\\nFull Name: Amanda Rodriguez\\nSkills: Data Security, Compliance, Healthcare Regulations, Azure\\nProjects: BetaHealth Secure Healthcare Data Analytics Platform on Azure\\n\\nFull Name: Jason Mitchell\\nSkills: Data Analytics, Machine Learning, Azure, Data Warehousing\\nProjects: GammaTech Smart Logistics Platform on Azure\\n\\nFull Name: Emily Turner\\nSkills: IoT, Real-time Data Management, Azure, Python\\nProjects: GammaTech Smart Logistics Platform on Azure\\n\\nFull Name: Michael Clark\\nSkills: Data Engineering, Data Warehousing, AWS, Python\\nProjects: AlphaCorp AWS-Powered Supply Chain Optimization Platform\\n\\nFull Name: Jessica White\\nSkills: Data Privacy, Security Compliance, Azure Key Vault, Healthcare Regulations\\nProjects: BetaHealth Secure Healthcare Data Analytics Platform on Azure\\n\\nFull Name: Daniel Brown\\nSkills: Cloud Architecture, DevOps, AWS Lambda, Azure Functions\\nProjects: GammaTech Smart Logistics Platform on Azure\\n\\nFull Name: Olivia Martinez\\nSkills: Real-time Monitoring, Azure Monitoring, Data Analytics\\nProjects: AlphaCorp AWS-Powered Supply Chain Optimization Platform\\n\\nFull Name: William Lee\\nSkills: Predictive Modeling, Machine Learning, AWS SageMaker, Azure Machine Learning\\nProjects: BetaHealth Secure Healthcare Data Analytics Platform on Azure<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHere is the output in the required format:\\n\\n{\\n    \"entities\": [\\n        {\"label\": \"Person\", \"id\": \"sarahjohnson\", \"name\": \"Sarah Johnson\"},\\n        {\"label\": \"Person\", \"id\": \"davidpatel\", \"name\": \"David Patel\"},\\n        {\"label\": \"Person\", \"id\": \"amandarodriguez\", \"name\": \"Amanda Rodriguez\"},\\n        {\"label\": \"Person\", \"id\": \"jasonmitchell\", \"name\": \"Jason Mitchell\"},\\n        {\"label\": \"Person\", \"id\": \"emilyturner\", \"name\": \"Emily Turner\"},\\n        {\"label\": \"Person\", \"id\": \"michaelclark\", \"name\": \"Michael Clark\"},\\n        {\"label\": \"Person\", \"id\": \"jessicawhite\", \"name\": \"Jessica White\"},\\n        {\"label\": \"Person\", \"id\": \"danielbrown\", \"name\": \"Daniel Brown\"},\\n        {\"label\": \"Person\", \"id\": \"oliviamartinez\", \"name\": \"Olivia Martinez\"},\\n        {\"label\": \"Person\", \"id\": \"williamlee\", \"name\": \"William Lee\"},\\n        {\"label\": \"'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test 2\n",
    "people_prompt_template = \"\"\"\n",
    "From the list of people below, extract the following Entities & relationships described in the mentioned format \n",
    "0. ALWAYS FINISH THE OUTPUT. Never send partial responses\n",
    "1. First, look for these Entity types in the text and generate as comma-separated format similar to entity type.\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. Do not create new entity types that aren't mentioned below. You will have to generate as many entities as needed as per the types below:\n",
    "    Entity Types:\n",
    "    label:'Project',id:string,name:string;summary:string //Project mentioned in the profile; `id` property is the full lowercase name of the project, with no capital letters, special characters, spaces or hyphens.\n",
    "    label:'Technology',id:string,name:string //Technology Entity, as listed in the \"skills\"-section of every person; `id` property is the name of the technology, in camel-case.\n",
    "    \n",
    "3. Next generate each relationships as triples of head, relationship and tail. To refer the head and tail entity, use their respective `id` property. Relationship property should be mentioned within brackets as comma-separated. They should follow these relationship types below. You will have to generate as many relationships as needed as defined below:\n",
    "    Relationship types:\n",
    "    person|HAS_SKILLS|technology \n",
    "    project|HAS_PEOPLE|person\n",
    "\n",
    "\n",
    "The output should look like :\n",
    "{\n",
    "    \"entities\": [{\"label\":\"Person\",\"id\":string,\"name\":string}],\n",
    "    \"relationships\": [\"projectid|HAS_PEOPLE|personid\"]\n",
    "}\n",
    "\n",
    "Case Sheet:\n",
    "Full Name: Sarah Johnson\n",
    "Skills: Machine Learning, Data Analytics, Azure, Python\n",
    "Projects: BetaHealth Secure Healthcare Data Analytics Platform on Azure\n",
    "\n",
    "Full Name: David Patel\n",
    "Skills: AWS, Cloud Computing, DevOps, Data Warehousing\n",
    "Projects: \n",
    "\n",
    "Full Name: Amanda Rodriguez\n",
    "Skills: Data Security, Compliance, Healthcare Regulations, Azure\n",
    "Projects: BetaHealth Secure Healthcare Data Analytics Platform on Azure\n",
    "\n",
    "Full Name: Jason Mitchell\n",
    "Skills: Data Analytics, Machine Learning, Azure, Data Warehousing\n",
    "Projects: GammaTech Smart Logistics Platform on Azure\n",
    "\n",
    "Full Name: Emily Turner\n",
    "Skills: IoT, Real-time Data Management, Azure, Python\n",
    "Projects: GammaTech Smart Logistics Platform on Azure\n",
    "\n",
    "Full Name: Michael Clark\n",
    "Skills: Data Engineering, Data Warehousing, AWS, Python\n",
    "Projects: AlphaCorp AWS-Powered Supply Chain Optimization Platform\n",
    "\n",
    "Full Name: Jessica White\n",
    "Skills: Data Privacy, Security Compliance, Azure Key Vault, Healthcare Regulations\n",
    "Projects: BetaHealth Secure Healthcare Data Analytics Platform on Azure\n",
    "\n",
    "Full Name: Daniel Brown\n",
    "Skills: Cloud Architecture, DevOps, AWS Lambda, Azure Functions\n",
    "Projects: GammaTech Smart Logistics Platform on Azure\n",
    "\n",
    "Full Name: Olivia Martinez\n",
    "Skills: Real-time Monitoring, Azure Monitoring, Data Analytics\n",
    "Projects: AlphaCorp AWS-Powered Supply Chain Optimization Platform\n",
    "\n",
    "Full Name: William Lee\n",
    "Skills: Predictive Modeling, Machine Learning, AWS SageMaker, Azure Machine Learning\n",
    "Projects: BetaHealth Secure Healthcare Data Analytics Platform on Azure\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful IT-project and account management expert who extracts information from documents.\"},\n",
    "    {\"role\": \"user\", \"content\": people_prompt_template},\n",
    "]\n",
    "\n",
    "start = timer()\n",
    "results = []\n",
    "reset_cache()\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])\n",
    "\n",
    "end = timer()\n",
    "print(f\"Pipeline completed in {end-start} seconds\")\n",
    "\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test 3\n",
    "\n",
    "prompt_temp = \"\"\"You are a seasoned stock market analyst. Your task is to create strategic questions for companies based on relevant news and basic financials from the past weeks, then provide an answer for the questions for the companies' financial status.\n",
    "0. ALWAYS FINISH THE OUTPUT. Never send partial responses\n",
    "1. The output should look like :\n",
    "{\n",
    "    \"question\": \"Where are Apple Inc. offices?\",\n",
    "    \"answer\": \"The offices are located in ONE APPLE PARK WAY Cupertino CA\"\n",
    "}\n",
    "\n",
    "Or like :\n",
    "{\n",
    "    \"question\": \"When where the stock value of Apple Inc. the highest?\",\n",
    "    \"answer\": \"Apple Inc. highest value was on 2023-12-14\"\n",
    "}\n",
    "2. Provide at least 20 and no more than 50 questions with their corresponding answers.\n",
    "\n",
    "$ctext\"\"\"\n",
    "\n",
    "# Read text from file\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read().replace('\\n', ' ')\n",
    "    return text\n",
    "\n",
    "def process_llama(file_prompt, system_msg):\n",
    "    reset_cache()\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": file_prompt},\n",
    "    ]\n",
    "    \n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    sleep(8)\n",
    "    return outputs[0][\"generated_text\"][len(prompt):]\n",
    "\n",
    "def extract_qa(file, prompt_template):\n",
    "    results = []\n",
    "    start = timer()\n",
    "\n",
    "    system_msg = \"Generate a question and answer pair based on the following news content.\"\n",
    "\n",
    "    # Read the text from a single news file\n",
    "    with open(file, 'r') as file:\n",
    "        text = file.read().replace('\\n', ' ')\n",
    "\n",
    "    print(\"Extracting questions and answers from the news content.\")\n",
    "    prompt = Template(prompt_template).substitute(ctext=json.dumps(text))\n",
    "    result = process_llama(prompt, system_msg=system_msg)\n",
    "    results.append(result)\n",
    "\n",
    "    end = timer()\n",
    "    print(f\"Pipeline completed in {end - start} seconds\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting questions and answers from the news content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (8192). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline completed in 8114.627988299999 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The date is 2024. The summary is 2024. The date is 2024. The headline is 2024. The date is 2024. The summary is 2024. The date is 2024. The date is 2024. The date is 2024. The summary is 2024. The date is 2024. The date is 2024. The date is 2024. The summary is 2024. The date is 2024. The summary is 2024. The date is 2024. The summary is 2024. The date is 2024. The date is 2024. The summary is 2024. The date is 2024. The date is 2024. The date is 2024. The summary is 2024. The date is 2024. The date is 2024. The date is 2024. The date is 2024. The summary is 2024. The date is 2024. The date is 2024. The date is 2024. The date is 2024. The date is 2024. The date is 2024. The date is ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path=\"./news.txt\"\n",
    "results= extract_qa(file_path,prompt_temp)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OpenAI' from 'openai' (c:\\Users\\jpbla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Generación del prompt base para pasar al LLM y genere nuestro data set base\u001b[39;00m\n\u001b[0;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m[INST]<<SYS>>\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124mYou are a seasoned stock market analyst. Your task is to create strategic questions for companies based on relevant news and basic financials from the past weeks, then provide an answer for the questions for the companies\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m financial status.\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m0. ALWAYS FINISH THE OUTPUT. Never send partial responses\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124m$ctext\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'OpenAI' from 'openai' (c:\\Users\\jpbla\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "#Generación del prompt base para pasar al LLM y genere nuestro data set base\n",
    "prompt_gpt = \"\"\"[INST]<<SYS>>\n",
    "You are a seasoned stock market analyst. Your task is to create strategic questions for companies based on relevant news and basic financials from the past weeks, then provide an answer for the questions for the companies' financial status.\n",
    "0. ALWAYS FINISH THE OUTPUT. Never send partial responses\n",
    "1. The output should look like :\n",
    "{\n",
    "    \"question\": \"Where are Apple Inc. offices?\",\n",
    "    \"answer\": \"The offices are located in ONE APPLE PARK WAY Cupertino CA\"\n",
    "}\n",
    "\n",
    "Or like :\n",
    "{\n",
    "    \"question\": \"When where the stock value of Apple Inc. the highest?\",\n",
    "    \"answer\": \"Apple Inc. highest value was on 2023-12-14\"\n",
    "}\n",
    "2. Provide at least 20 and no more than 50 questions with their corresponding answers.\n",
    "\n",
    "<<SYS>>\n",
    "$ctext\n",
    "\"\"\"\n",
    "\n",
    "def extract_qa_chatgpt(file, prompt_template):\n",
    "    results = []\n",
    "    start = timer()\n",
    "\n",
    "    system_msg = \"Generate a question and answer pair based on the following news content.\"\n",
    "\n",
    "    # Read the text from a single news file\n",
    "    with open(file, 'r') as file:\n",
    "        text = file.read().replace('\\n', ' ')\n",
    "\n",
    "    print(\"Extracting questions and answers from the news content.\")\n",
    "    prompt = Template(prompt_template).substitute(ctext=json.dumps(text))\n",
    "    result = process_gpt(prompt, system_msg=system_msg)\n",
    "    results.append(result)\n",
    "\n",
    "    end = timer()\n",
    "    print(f\"Pipeline completed in {end - start} seconds\")\n",
    "    return results\n",
    "\n",
    "client = OpenAI(api_key = \"\")\n",
    "\n",
    "def process_gpt(file_prompt, system_msg):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Specify the chat model you're using,\n",
    "        max_tokens=500,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": file_prompt},\n",
    "        ],\n",
    "    )\n",
    "    nlp_results = completion.choices[0].message.content\n",
    "    sleep(8)\n",
    "    return nlp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"./news.txt\"\n",
    "results_gpt= extract_qa_chatgpt(file_path,prompt_gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. ¿Qué algoritmo se puede utilizar como baseline para predecir las variables objetivo? \n",
    "\n",
    "En este caso se utilizan dos LLMs para generar las entradas y salidas el procedimiento es el siguiente:\n",
    "* Generar preguntas y respuestas con datos financieros utilizando Llama3 y Chatgpt4.\n",
    "* Entrenar el mismo LLM con las preguntas y respuestas de nuestros datos financieros.\n",
    "* Utilizar el RAG (LLM entrenado) para hacer pruebas de consultas.\n",
    "* Utilizar el RAG para generar las relaciones y nodos para la base de datos de grafos (graphRAG) en NEO4j.\n",
    "Como tal la hipotesis es que al preguntar al modelo en graphRAG sera mucho más rápido y mejor la respuesta.\n",
    "\n",
    "2. ¿Se puede determinar la importancia de las características para el modelo generado? Recuerden que incluir características irrelevantes puede afectar negativamente el rendimiento del modelo y aumentar la complejidad sin beneficios sustanciales.\n",
    "\n",
    "En este caso las caracteristicas seran determinadas por el mismo modelo.\n",
    "\n",
    "3. ¿El modelo está sub/sobreajustando los datos de entrenamiento?\n",
    "Seria sobre ajustado si no se responden las preguntas adecuadamente.\n",
    "\n",
    "4. ¿Cuál es la métrica adecuada para este problema de negocio? \n",
    "La metrica es la acertividad de la respuesta aunque se puden utilizar diversas metricas de la literatura que aun no tenemos definidas.\n",
    "\n",
    "5. ¿Cuál debería ser el desempeño mínimo a obtener?\n",
    "El desempeño minimo es que se responda la pregunta.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
